{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 决策树模型构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Decision_Tree_C45:\n",
    "    # define entropy calculation\n",
    "    def Entropy(self, train_data):\n",
    "        inst_num = len(train_data)  # instances number\n",
    "        label_counts = {}  # count instances of each class\n",
    "        for i in range(inst_num):\n",
    "            label = train_data[i][-1]  #get instance class\n",
    "            if label not in label_counts.keys():\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1  #count\n",
    "        ent = 0\n",
    "        for key in label_counts.keys():\n",
    "            #calculate each class proportion\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            ent -= prob * log(prob, 2)  # see Eq.(3.1)\n",
    "        return ent\n",
    "\n",
    "    #split data according to feature and feature value\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "    #choose best feature to split\n",
    "    def choose_split_feature(self, train_data):\n",
    "        feat_num = len(train_data[0]) - 1  # get available features\n",
    "        base_ent = self.Entropy(train_data)\n",
    "        bestInforGain = 0.0\n",
    "        best_feat_index = -1\n",
    "        best_feat_value = 0\n",
    "        for i in range(feat_num):\n",
    "            if isinstance(train_data[0][i], str):  #for discrete feature\n",
    "                feat_list = [example[i] for example in train_data]\n",
    "                unique_values = set(feat_list)\n",
    "                newEnt = 0\n",
    "                for value in unique_values:\n",
    "                    sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "                    prop = float(len(sub_data)) / len(train_data)\n",
    "                    newEnt += prop * self.Entropy(sub_data)  #see Eq.(3.2)\n",
    "                    inforgain = base_ent - newEnt\n",
    "                    if inforgain > bestInforGain:\n",
    "                        best_feat_index = i\n",
    "                        bestInforGain = inforgain\n",
    "                else:  #for continous feature\n",
    "                    feat_list = [example[i] for example in train_data]\n",
    "                    unique_values = set(feat_list)\n",
    "                    sort_unique_values = sorted(unique_values)\n",
    "                    minEnt = np.inf\n",
    "                    for j in range(len(sort_unique_values) - 1):\n",
    "                        div_value = (sort_unique_values[j] + sort_unique_values[j + 1]) / 2\n",
    "                        sub_data_left = self.split_data(train_data, i, div_value, \"L\")\n",
    "                        sub_data_right = self.split_data(train_data, i, div_value, \"R\")\n",
    "                        prop_left = float(len(sub_data_left)) / len(train_data)\n",
    "                        prop_right = float(len(sub_data_right)) / len(train_data)\n",
    "                        ent = prop_left * self.Entropy(sub_data_left) + \\\n",
    "                              prop_right * self.Entropy(sub_data_right)  #see Eq.(3.6)\n",
    "                        if ent < minEnt:\n",
    "                            minEnt = ent\n",
    "                            best_feat_value = div_value\n",
    "                    inforgain = base_ent - minEnt\n",
    "                    if inforgain > bestInforGain:\n",
    "                        bestInforGain = inforgain\n",
    "                        best_feat_index = i\n",
    "            return best_feat_index, best_feat_value\n",
    "\n",
    "    # get major class\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount.key():\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "            sortedclasscount = sorted(classcount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "            major = sortedclasscount[0][0]\n",
    "            return major\n",
    "\n",
    "    # create decision tree\n",
    "    def create_decision_tree(self, train_data, feat_names):\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):  #see condition A\n",
    "            return classList[0]\n",
    "        if len(train_data[0]) == 1:  #see condition B\n",
    "            return self.get_major_class(classList)\n",
    "        if len(train_data) == 0:  #see condition C\n",
    "            return\n",
    "        # choose best division feature\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "        if isinstance(train_data[0][best_feat], str):  # for discrete feature\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}  # generate a root node\n",
    "            del (feat_names[best_feat])  # del feature used\n",
    "            feat_values = [example[best_feat] for example in train_data]\n",
    "            unique_feat_values = set(feat_values)\n",
    "            #create a node for each value of the best feature\n",
    "            for value in unique_feat_values:\n",
    "                sub_feat_names = feat_names[:]\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                         best_feat, value, \"D\"),\n",
    "                                                                         sub_feat_names)\n",
    "        else:  #for contiunous feature\n",
    "            best_feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "            tree_model = {best_feat_name: {}}  # generate a root node\n",
    "            sub_feat_names = feat_names\n",
    "            # generate left node\n",
    "            tree_model[best_feat_name][\"Y\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"L\"),\n",
    "                                                                        sub_feat_names)\n",
    "            #generate right node\n",
    "            tree_model[best_feat_name][\"N\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"R\"),\n",
    "                                                                        sub_feat_names)\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 案例实践"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision_Tree_C45 accuracy:0.760\n",
      "Decision_Tree_SKL accuracy:0.757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#导入数据集\n",
    "#Add code here to load data from file and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Client_Info.csv\", encoding='gb2312')\n",
    "data = np.array(data)\n",
    "feat_names = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2022)\n",
    "\n",
    "#Add code here to build decision tree model using Decision_Tree_C45\n",
    "model = Decision_Tree_C45()\n",
    "tree = model.create_decision_tree(train, feat_names)\n",
    "pred_labels = []\n",
    "for i in range(len(test)):\n",
    "    label = model.predict(tree, feat_names, test[i])\n",
    "    pred_labels.append(label)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_C45 accuracy:%.3f\" % (acc / len(test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# s-klearn包\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Add code here to build decision tree model using sklearn\n",
    "model_skl = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_skl.fit(train[:, 0:-1], train[:, -1])\n",
    "pred_labels_skl = model_skl.predict(test[:, 0:-1])\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_skl[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_SKL accuracy:%.3f\" % (acc / len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 拓展训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Decision_Tree_C45:\n",
    "    def __init__(self, max_depth=float('inf'), min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion  # \"gini\" or \"entropy\"\n",
    "\n",
    "    # define entropy calculation\n",
    "    def Entropy(self, train_data):\n",
    "        inst_num = len(train_data)  # instances number\n",
    "        label_counts = {}  # count instances of each class\n",
    "        for i in range(inst_num):\n",
    "\n",
    "            label = train_data[i][-1]  #get instance class\n",
    "            if label not in label_counts.keys():\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1  #count\n",
    "        ent = 0\n",
    "        for key in label_counts.keys():\n",
    "            #calculate each class proportion\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            ent -= prob * log(prob, 2)  # see Eq.(3.1)\n",
    "        return ent\n",
    "        #split data according to feature and feature value\n",
    "\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "    # #choose best feature to split\n",
    "    # def choose_split_feature(self, train_data, depth):\n",
    "    #     # 检查最大深度和最小样本数的条件\n",
    "    #     if depth >= self.max_depth or len(train_data) < self.min_samples_split:\n",
    "    #         return None, None\n",
    "    #\n",
    "    #     # 选择划分标准：基尼指数或熵\n",
    "    #     if self.criterion == 'gini':\n",
    "    #         base_measure = self.Gini(train_data)\n",
    "    #     else:\n",
    "    #         base_measure = self.Entropy(train_data)\n",
    "    #\n",
    "    #     feat_num = len(train_data[0]) - 1  # get available features\n",
    "    #     base_ent = self.Entropy(train_data)\n",
    "    #     bestInforGain = 0.0\n",
    "    #     best_feat_index = -1\n",
    "    #     best_feat_value = 0\n",
    "    #     for i in range(feat_num):\n",
    "    #         if isinstance(train_data[0][i], str):  #for discrete feature\n",
    "    #             feat_list = [example[i] for example in train_data]\n",
    "    #             unique_values = set(feat_list)\n",
    "    #             newEnt = 0\n",
    "    #             for value in unique_values:\n",
    "    #                 sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "    #                 prop = float(len(sub_data)) / len(train_data)\n",
    "    #                 newEnt += prop * self.Entropy(sub_data)  #see Eq.(3.2)\n",
    "    #                 inforgain = base_ent - newEnt\n",
    "    #                 if inforgain > bestInforGain:\n",
    "    #                     best_feat_index = i\n",
    "    #                     bestInforGain = inforgain\n",
    "    #             else:  #for continous feature\n",
    "    #                 feat_list = [example[i] for example in train_data]\n",
    "    #                 unique_values = set(feat_list)\n",
    "    #                 sort_unique_values = sorted(unique_values)\n",
    "    #                 minEnt = np.inf\n",
    "    #                 for j in range(len(sort_unique_values) - 1):\n",
    "    #                     div_value = (sort_unique_values[j] + sort_unique_values[j + 1]) / 2\n",
    "    #                     sub_data_left = self.split_data(train_data, i, div_value, \"L\")\n",
    "    #                     sub_data_right = self.split_data(train_data, i, div_value, \"R\")\n",
    "    #                     prop_left = float(len(sub_data_left)) / len(train_data)\n",
    "    #                     prop_right = float(len(sub_data_right)) / len(train_data)\n",
    "    #                     ent = prop_left * self.Entropy(sub_data_left) + \\\n",
    "    #                           prop_right * self.Entropy(sub_data_right)  #see Eq.(3.6)\n",
    "    #                     if ent < minEnt:\n",
    "    #                         minEnt = ent\n",
    "    #                         best_feat_value = div_value\n",
    "    #                 inforgain = base_ent - minEnt\n",
    "    #                 if inforgain > bestInforGain:\n",
    "    #                     bestInforGain = inforgain\n",
    "    #                     best_feat_index = i\n",
    "    #         return best_feat_index, best_feat_value\n",
    "\n",
    "\n",
    "    def choose_split_feature(self, train_data, depth):\n",
    "        # 检查最大深度和最小样本数的条件\n",
    "        if depth >= self.max_depth or len(train_data) < self.min_samples_split:\n",
    "            return None, None\n",
    "\n",
    "        feat_num = len(train_data[0]) - 1  # 获取可用特征的数量\n",
    "        bestInforGain = 0.0\n",
    "        best_feat_index = -1\n",
    "        best_feat_value = None\n",
    "\n",
    "        # 根据选择的标准计算基础度量（基尼指数或熵）\n",
    "        if self.criterion == 'gini':\n",
    "            base_measure = self.Gini(train_data)\n",
    "        else:\n",
    "            base_measure = self.Entropy(train_data)\n",
    "\n",
    "        for i in range(feat_num):\n",
    "            feat_list = [example[i] for example in train_data]\n",
    "            unique_values = set(feat_list)\n",
    "\n",
    "            for value in unique_values:\n",
    "                sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "                prop = float(len(sub_data)) / len(train_data)\n",
    "                if self.criterion == 'gini':\n",
    "                    new_measure = prop * self.Gini(sub_data)\n",
    "                else:\n",
    "                    new_measure = prop * self.Entropy(sub_data)\n",
    "\n",
    "                inforgain = base_measure - new_measure\n",
    "                if inforgain > bestInforGain:\n",
    "                    bestInforGain = inforgain\n",
    "                    best_feat_index = i\n",
    "                    best_feat_value = value\n",
    "\n",
    "        return best_feat_index, best_feat_value\n",
    "\n",
    "    # get major class\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount:\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "\n",
    "        sortedclasscount = sorted(classcount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        major = sortedclasscount[0][0]\n",
    "        return major\n",
    "\n",
    "    # # create decision tree\n",
    "    # def create_decision_tree(self, train_data, feat_names):\n",
    "    #     classList = [example[-1] for example in train_data]\n",
    "    #     if classList.count(classList[0]) == len(classList):  #see condition A\n",
    "    #         return classList[0]\n",
    "    #     if len(train_data[0]) == 1:  #see condition B\n",
    "    #         return self.get_major_class(classList)\n",
    "    #     if len(train_data) == 0:  #see condition C\n",
    "    #         return\n",
    "    #     # choose best division feature\n",
    "    #     best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "    #     if isinstance(train_data[0][best_feat], str):  # for discrete feature\n",
    "    #         feat_name = feat_names[best_feat]\n",
    "    #         tree_model = {feat_name: {}}  # generate a root node\n",
    "    #         del (feat_names[best_feat])  # del feature used\n",
    "    #         feat_values = [example[best_feat] for example in train_data]\n",
    "    #         unique_feat_values = set(feat_values)\n",
    "    #         #create a node for each value of the best feature\n",
    "    #         for value in unique_feat_values:\n",
    "    #             sub_feat_names = feat_names[:]\n",
    "    #             tree_model[feat_name][value] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                      best_feat, value, \"D\"),\n",
    "    #                                                                      sub_feat_names)\n",
    "    #     else:  #for contiunous feature\n",
    "    #         best_feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "    #         tree_model = {best_feat_name: {}}  # generate a root node\n",
    "    #         sub_feat_names = feat_names\n",
    "    #         # generate left node\n",
    "    #         tree_model[best_feat_name][\"Y\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                     best_feat,\n",
    "    #                                                                                     best_div_value, \"L\"),\n",
    "    #                                                                     sub_feat_names)\n",
    "    #         #generate right node\n",
    "    #         tree_model[best_feat_name][\"N\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                     best_feat,\n",
    "    #                                                                                     best_div_value, \"R\"),\n",
    "    #                                                                     sub_feat_names)\n",
    "    #     return tree_model\n",
    "    def create_decision_tree(self, train_data, feat_names, depth=0):\n",
    "        # 检查是否所有的类标签都相同\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "\n",
    "        # 检查是否还有可用的特征或者是否达到最大深度\n",
    "        if len(train_data[0]) == 1 or depth >= self.max_depth:\n",
    "            return self.get_major_class(classList)\n",
    "\n",
    "        # 选择最佳分割特征\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "        if best_feat is None:  # 如果没有更多特征可用于进一步分割\n",
    "            return self.get_major_class(classList)\n",
    "\n",
    "        # 根据最佳特征的类型进行分割\n",
    "        if isinstance(train_data[0][best_feat], str):  # 离散特征\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}\n",
    "            del feat_names[best_feat]  # 删除已使用的特征\n",
    "            unique_feat_values = set([example[best_feat] for example in train_data])\n",
    "            for value in unique_feat_values:\n",
    "                sub_feat_names = feat_names[:]  # 复制特征列表\n",
    "                sub_data = self.split_data(train_data, best_feat, value, \"D\")\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(sub_data, sub_feat_names, depth + 1)\n",
    "        else:  # 连续特征\n",
    "            feat_name = feat_names[best_feat] + \"<=\" + str(best_div_value)\n",
    "            tree_model = {feat_name: {}}\n",
    "            sub_feat_names = feat_names[:]\n",
    "            # 分割为左右子树\n",
    "            left_subtree = self.split_data(train_data, best_feat, best_div_value, \"L\")\n",
    "            right_subtree = self.split_data(train_data, best_feat, best_div_value, \"R\")\n",
    "            tree_model[feat_name][\"yes\"] = self.create_decision_tree(left_subtree, sub_feat_names, depth + 1)\n",
    "            tree_model[feat_name][\"no\"] = self.create_decision_tree(right_subtree, sub_feat_names, depth + 1)\n",
    "\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n",
    "\n",
    "    def Gini(self, train_data):\n",
    "        inst_num = len(train_data)\n",
    "        label_counts = {}\n",
    "        for feat_vect in train_data:\n",
    "            label = feat_vect[-1]\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1\n",
    "        gini = 1.0\n",
    "        for key in label_counts:\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            gini -= prob ** 2\n",
    "        return gini\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "choose_split_feature() missing 1 required positional argument: 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#Add code here to build decision tree model using Decision_Tree_C45\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m Decision_Tree_C45(max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, min_samples_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, criterion\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgini\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m tree \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# model = Decision_Tree_C45()\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# tree = model.create_decision_tree(train, feat_names)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m pred_labels \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[0;32mIn[33], line 203\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_major_class(classList)\n\u001B[1;32m    202\u001B[0m \u001B[38;5;66;03m# 选择最佳分割特征\u001B[39;00m\n\u001B[0;32m--> 203\u001B[0m best_feat, best_div_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_split_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_feat \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# 如果没有更多特征可用于进一步分割\u001B[39;00m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_major_class(classList)\n",
      "\u001B[0;31mTypeError\u001B[0m: choose_split_feature() missing 1 required positional argument: 'depth'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Add code here to load data from file and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Client_Info.csv\", encoding='gb2312')\n",
    "data = np.array(data)\n",
    "feat_names = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2021)\n",
    "#Add code here to build decision tree model using Decision_Tree_C45\n",
    "model = Decision_Tree_C45(max_depth=5, min_samples_split=10, criterion='gini')\n",
    "tree = model.create_decision_tree(train, feat_names)\n",
    "# model = Decision_Tree_C45()\n",
    "# tree = model.create_decision_tree(train, feat_names)\n",
    "pred_labels = []\n",
    "for i in range(len(test)):\n",
    "    label = model.predict(tree, feat_names, test[i])\n",
    "    pred_labels.append(label)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_C45 accuracy:%.3f\" % (acc / len(test)))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Add code here to build decision tree model using sklearn\n",
    "model_skl = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_skl.fit(train[:, 0:-1], train[:, -1])\n",
    "pred_labels_skl = model_skl.predict(test[:, 0:-1])\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_skl[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_SKL accuracy:%.3f\" % (acc / len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-1][-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第一个控制行，第二个控制列（-1代表最后一列）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
