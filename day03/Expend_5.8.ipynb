{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 拓展训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Decision_Tree_C45:\n",
    "    # define entropy calculation\n",
    "    def Entropy(self, train_data):\n",
    "        inst_num = len(train_data)  # instances number\n",
    "        label_counts = {}  # count instances of each class\n",
    "        for i in range(inst_num):\n",
    "            label = train_data[i][-1]  #get instance class\n",
    "            if label not in label_counts.keys():\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1  #count\n",
    "        ent = 0\n",
    "        for key in label_counts.keys():\n",
    "            #calculate each class proportion\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            ent -= prob * log(prob, 2)  # see Eq.(3.1)\n",
    "        return ent\n",
    "\n",
    "    #split data according to feature and feature value\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "    #choose best feature to split\n",
    "    def choose_split_feature(self, train_data):\n",
    "        feat_num = len(train_data[0]) - 1  # get available features\n",
    "        base_ent = self.Entropy(train_data)\n",
    "        bestInforGain = 0.0\n",
    "        best_feat_index = -1\n",
    "        best_feat_value = 0\n",
    "        for i in range(feat_num):\n",
    "            if isinstance(train_data[0][i], str):  #for discrete feature\n",
    "                feat_list = [example[i] for example in train_data]\n",
    "                unique_values = set(feat_list)\n",
    "                newEnt = 0\n",
    "                for value in unique_values:\n",
    "                    sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "                    prop = float(len(sub_data)) / len(train_data)\n",
    "                    newEnt += prop * self.Entropy(sub_data)  #see Eq.(3.2)\n",
    "                inforgain = base_ent - newEnt\n",
    "                if inforgain > bestInforGain:\n",
    "                    best_feat_index = i\n",
    "                    bestInforGain = inforgain\n",
    "            else:  #for continous feature\n",
    "                feat_list = [example[i] for example in train_data]\n",
    "                unique_values = set(feat_list)\n",
    "                sort_unique_values = sorted(unique_values)\n",
    "                minEnt = np.inf\n",
    "                for j in range(len(sort_unique_values) - 1):\n",
    "                    div_value = (sort_unique_values[j] + sort_unique_values[j + 1]) / 2\n",
    "                    sub_data_left = self.split_data(train_data, i, div_value, \"L\")\n",
    "                    sub_data_right = self.split_data(train_data, i, div_value, \"R\")\n",
    "                    prop_left = float(len(sub_data_left)) / len(train_data)\n",
    "                    prop_right = float(len(sub_data_right)) / len(train_data)\n",
    "                    ent = prop_left * self.Entropy(sub_data_left) + \\\n",
    "                          prop_right * self.Entropy(sub_data_right)  #see Eq.(3.6)\n",
    "                    if ent < minEnt:\n",
    "                        minEnt = ent\n",
    "                        best_feat_value = div_value\n",
    "                inforgain = base_ent - minEnt\n",
    "                if inforgain > bestInforGain:\n",
    "                    bestInforGain = inforgain\n",
    "                    best_feat_index = i\n",
    "            return best_feat_index, best_feat_value\n",
    "\n",
    "    # get major class\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount.keys():\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "            sortedclasscount = sorted(classcount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            major = sortedclasscount[0][0]\n",
    "            return major\n",
    "\n",
    "    # create decision tree\n",
    "    def create_decision_tree(self, train_data, feat_names):\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):  #see condition A\n",
    "            return classList[0]\n",
    "        if len(train_data[0]) == 1:  #see condition B\n",
    "            return self.get_major_class(classList)\n",
    "        if len(train_data) == 0:  #see condition C\n",
    "            return\n",
    "        # choose best division feature\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "        if isinstance(train_data[0][best_feat], str):  # for discrete feature\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}  # generate a root node\n",
    "            del (feat_names[best_feat])  # del feature used\n",
    "            feat_values = [example[best_feat] for example in train_data]\n",
    "            unique_feat_values = set(feat_values)\n",
    "            #create a node for each value of the best feature\n",
    "            for value in unique_feat_values:\n",
    "                sub_feat_names = feat_names[:]\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                         best_feat, value, \"D\"),\n",
    "                                                                         sub_feat_names)\n",
    "        else:  #for contiunous feature\n",
    "            best_feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "            tree_model = {best_feat_name: {}}  # generate a root node\n",
    "            sub_feat_names = feat_names\n",
    "            # generate left node\n",
    "            tree_model[best_feat_name][\"Y\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"L\"),\n",
    "                                                                        sub_feat_names)\n",
    "            #generate right node\n",
    "            tree_model[best_feat_name][\"N\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"R\"),\n",
    "                                                                        sub_feat_names)\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Decision_Tree_Gini:\n",
    "    #基尼指数\n",
    "    def Gini(self, train_data):\n",
    "        # 初始化基尼指数为 1\n",
    "        gini = 1.0\n",
    "\n",
    "        # 初始化类别计数字典\n",
    "        label_counts = {}\n",
    "        for example in train_data:\n",
    "            label = example[-1]  # 获取样本的类别标签\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1\n",
    "\n",
    "        # 计算每个类别的出现概率并计算基尼指数\n",
    "        for label in label_counts:\n",
    "            prob = float(label_counts[label]) / len(train_data)\n",
    "            gini -= prob**2\n",
    "\n",
    "        return gini\n",
    "\n",
    "    #分割数据集指定分割特征\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "\n",
    "\n",
    "    #选择最佳特征用于分割数据\n",
    "    def choose_split_feature(self, train_data):\n",
    "        feat_num = len(train_data[0]) - 1  # get available features\n",
    "        global bestInforGini\n",
    "        bestInforGini = np.inf\n",
    "        best_feat_index = -1\n",
    "        best_feat_value = 0\n",
    "        for i in range(feat_num):\n",
    "            if isinstance(train_data[0][i], str):  #for discrete feature\n",
    "                feat_list = [example[i] for example in train_data]\n",
    "                unique_values = set(feat_list)\n",
    "                newGini = 0\n",
    "                for value in unique_values:\n",
    "                    sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "                    prop = float(len(sub_data)) / len(train_data)\n",
    "                    newGini += prop * self.Gini(sub_data)  #see Eq.(3.2)\n",
    "                if newGini < bestInforGini:\n",
    "                    best_feat_index = i\n",
    "                    bestInforGini = newGini\n",
    "            else:  #for continous feature\n",
    "                feat_list = [example[i] for example in train_data]\n",
    "                unique_values = set(feat_list)\n",
    "                sort_unique_values = sorted(unique_values)\n",
    "                minGini = np.inf\n",
    "                for j in range(len(sort_unique_values) - 1):\n",
    "                    div_value = (sort_unique_values[j] + sort_unique_values[j + 1]) / 2\n",
    "                    sub_data_left = self.split_data(train_data, i, div_value, \"L\")\n",
    "                    sub_data_right = self.split_data(train_data, i, div_value, \"R\")\n",
    "                    prop_left = float(len(sub_data_left)) / len(train_data)\n",
    "                    prop_right = float(len(sub_data_right)) / len(train_data)\n",
    "                    gini = prop_left * self.Gini(sub_data_left) + \\\n",
    "                          prop_right * self.Gini(sub_data_right)  #see Eq.(3.6)\n",
    "                    if gini < minGini:\n",
    "                        minGini = gini\n",
    "                        best_feat_value = div_value\n",
    "                if minGini < bestInforGini:\n",
    "                    bestInforGini = minGini\n",
    "                    best_feat_index = i\n",
    "            return best_feat_index, best_feat_value\n",
    "\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount.keys():\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "\n",
    "        sortedclasscount = sorted(classcount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        major = sortedclasscount[0][0]\n",
    "        return major\n",
    "\n",
    "    # create decision tree\n",
    "    def create_decision_tree(self, train_data, feat_names,depth=0,max_depth=None, min_samples=2):\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):  #see condition A\n",
    "            return classList[0]\n",
    "        # 检查是否没有更多特征可用或达到最大深度\n",
    "        if len(train_data[0]) == 1 or (max_depth is not None and depth >= max_depth):\n",
    "            return self.get_major_class(classList)\n",
    "        # 检查样本数是否少于最小样本个数\n",
    "        if len(train_data) < min_samples:\n",
    "            return self.get_major_class([example[-1] for example in train_data])\n",
    "        if len(train_data[0]) == 1:  #see condition B\n",
    "            return self.get_major_class(classList)\n",
    "        if len(train_data) == 0:  #see condition C\n",
    "            return\n",
    "        # choose best division feature\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "        if isinstance(train_data[0][best_feat], str):  # for discrete feature\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}  # generate a root node\n",
    "            del (feat_names[best_feat])  # del feature used\n",
    "            feat_values = [example[best_feat] for example in train_data]\n",
    "            unique_feat_values = set(feat_values)\n",
    "            #create a node for each value of the best feature\n",
    "            for value in unique_feat_values:\n",
    "                sub_feat_names = feat_names[:]\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                         best_feat, value, \"D\"),\n",
    "                                                                         sub_feat_names,depth + 1,max_depth=max_depth,min_samples=min_samples)\n",
    "        else:  #for contiunous feature\n",
    "            best_feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "            tree_model = {best_feat_name: {}}  # generate a root node\n",
    "            sub_feat_names = feat_names\n",
    "            # generate left node\n",
    "            tree_model[best_feat_name][\"Y\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"L\"),\n",
    "                                                                        sub_feat_names,depth + 1,max_depth=max_depth,min_samples=min_samples)\n",
    "            #generate right node\n",
    "            tree_model[best_feat_name][\"N\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "                                                                                        best_feat,\n",
    "                                                                                        best_div_value, \"R\"),\n",
    "                                                                        sub_feat_names,depth + 1,max_depth=max_depth,min_samples=min_samples)\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        if not isinstance(tree_model, dict) or not tree_model:\n",
    "            # 如果决策树模型为空或不是字典类型，返回默认值或错误信息\n",
    "            return 'Unknown'\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision_Tree_C45 accuracy:0.650\n",
      "Decision_Tree_Gini accuracy:0.740\n",
      "Decision_Tree_SKL accuracy:0.787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#导入数据集\n",
    "#Add code here to load data from file and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Client_Info.csv\", encoding='gb2312')\n",
    "data = np.array(data)\n",
    "feat_names = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2024)\n",
    "\n",
    "model_C45 = Decision_Tree_C45()\n",
    "tree_C45 = model_C45.create_decision_tree(train, feat_names)\n",
    "pred_labels_C45 = []\n",
    "for i in range(len(test)):\n",
    "    label_C45 = model_C45.predict(tree_C45, feat_names, test[i])\n",
    "    pred_labels_C45.append(label_C45)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_C45[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_C45 accuracy:%.3f\" % (acc / len(test)))\n",
    "\n",
    "\n",
    "#Add code here to build decision tree model using Decision_Tree_C45\n",
    "model = Decision_Tree_Gini()\n",
    "tree = model.create_decision_tree(train, feat_names,max_depth=5,min_samples=2)\n",
    "pred_labels = []\n",
    "for i in range(len(test)):\n",
    "    label = model.predict(tree, feat_names, test[i])\n",
    "    pred_labels.append(label)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_Gini accuracy:%.3f\" % (acc / len(test)))\n",
    "\n",
    "# s-klearn包\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Add code here to build decision tree model using sklearn\n",
    "model_skl = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_skl.fit(train[:, 0:-1], train[:, -1])\n",
    "pred_labels_skl = model_skl.predict(test[:, 0:-1])\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_skl[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_SKL accuracy:%.3f\" % (acc / len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
