{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 拓展训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "class Decision_Tree_C45:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "    # define entropy calculation\n",
    "    def Entropy(self, train_data):\n",
    "        inst_num = len(train_data)  # instances number\n",
    "        label_counts = {}  # count instances of each class\n",
    "        for i in range(inst_num):\n",
    "            label = train_data[i][-1]  #get instance class\n",
    "            if label not in label_counts.keys():\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1  #count\n",
    "        ent = 0\n",
    "        for key in label_counts.keys():\n",
    "            #calculate each class proportion\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            ent -= prob * log(prob, 2)  # see Eq.(3.1)\n",
    "        return ent\n",
    "\n",
    "    def Gini(self, train_data):\n",
    "        inst_num = len(train_data)\n",
    "        label_counts = {}\n",
    "        for feat_vect in train_data:\n",
    "            label = feat_vect[-1]\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1\n",
    "        gini = 1.0\n",
    "        for key in label_counts:\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            gini -= prob ** 2\n",
    "        return gini\n",
    "\n",
    "\n",
    "    #split data according to feature and feature value\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "    def choose_split_feature(self, train_data):\n",
    "        num_features = len(train_data[0]) - 1  # 特征数量\n",
    "        best_gini = 1.0\n",
    "        best_info_gain = 0.0\n",
    "        best_feature = -1\n",
    "        best_split_val = None\n",
    "\n",
    "        for i in range(num_features):\n",
    "            feature_values = set([example[i] for example in train_data])\n",
    "            for split_val in feature_values:\n",
    "                left_subset = self.split_data(train_data, i, split_val, \"L\")\n",
    "                right_subset = self.split_data(train_data, i, split_val, \"R\")\n",
    "                if len(left_subset) < 1 or len(right_subset) < 1:\n",
    "                    continue\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    new_gini = self.calculate_gini_index(left_subset, right_subset)\n",
    "                    if new_gini < best_gini:\n",
    "                        best_gini = new_gini\n",
    "                        best_feature = i\n",
    "                        best_split_val = split_val\n",
    "                else:\n",
    "                    new_info_gain = self.calculate_info_gain(train_data, left_subset, right_subset)\n",
    "                    if new_info_gain > best_info_gain:\n",
    "                        best_info_gain = new_info_gain\n",
    "                        best_feature = i\n",
    "                        best_split_val = split_val\n",
    "\n",
    "        return best_feature, best_split_val\n",
    "\n",
    "    def calculate_gini_index(self, left_subset, right_subset):\n",
    "        total_size = len(left_subset) + len(right_subset)\n",
    "        gini_left = self.Gini(left_subset)\n",
    "        gini_right = self.Gini(right_subset)\n",
    "        weighted_gini = (len(left_subset) / total_size) * gini_left + (len(right_subset) / total_size) * gini_right\n",
    "        return weighted_gini\n",
    "\n",
    "    def calculate_info_gain(self, parent_set, left_subset, right_subset):\n",
    "        parent_entropy = self.Entropy(parent_set)\n",
    "        total_size = len(parent_set)\n",
    "        left_entropy = self.Entropy(left_subset)\n",
    "        right_entropy = self.Entropy(right_subset)\n",
    "        weighted_entropy = (len(left_subset) / total_size) * left_entropy + (len(right_subset) / total_size) * right_entropy\n",
    "        info_gain = parent_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    # get major class\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount.key():\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "            sortedclasscount = sorted(classcount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "            major = sortedclasscount[0][0]\n",
    "            return major\n",
    "\n",
    "    def create_decision_tree(self, train_data, feat_names, depth=0):\n",
    "        # 检查是否达到最大深度\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return self.get_major_class([example[-1] for example in train_data])\n",
    "\n",
    "        # 检查是否满足最小样本个数要求\n",
    "        if len(train_data) < self.min_samples_split:\n",
    "            return self.get_major_class([example[-1] for example in train_data])\n",
    "\n",
    "        # 检查是否所有实例属于同一类别\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "\n",
    "        # 检查是否没有更多特征\n",
    "        if len(train_data[0]) == 1:\n",
    "            return self.get_major_class(classList)\n",
    "\n",
    "        # 选择最佳分裂属性\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "\n",
    "        # 处理离散特征和连续特征的情况\n",
    "        if isinstance(train_data[0][best_feat], str):\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}\n",
    "            del (feat_names[best_feat])\n",
    "            unique_values = set([example[best_feat] for example in train_data])\n",
    "            for value in unique_values:\n",
    "                sub_feat_names = feat_names[:]\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(\n",
    "                    self.split_data(train_data, best_feat, value, \"D\"),\n",
    "                    sub_feat_names, depth + 1)\n",
    "        else:\n",
    "            feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "            tree_model = {feat_name: {}}\n",
    "            sub_feat_names = feat_names[:]\n",
    "            tree_model[feat_name][\"Y\"] = self.create_decision_tree(\n",
    "                self.split_data(train_data, best_feat, best_div_value, \"L\"),\n",
    "                sub_feat_names, depth + 1)\n",
    "            tree_model[feat_name][\"N\"] = self.create_decision_tree(\n",
    "                self.split_data(train_data, best_feat, best_div_value, \"R\"),\n",
    "                sub_feat_names, depth + 1)\n",
    "\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'key'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#Add code here to build decision tree model using Decision_Tree_C45\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m Decision_Tree_C45(max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, min_samples_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, criterion\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgini\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m tree \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# model = Decision_Tree_C45()\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# tree = model.create_decision_tree(train, feat_names)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m pred_labels \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[0;32mIn[10], line 158\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    156\u001B[0m     tree_model \u001B[38;5;241m=\u001B[39m {feat_name: {}}\n\u001B[1;32m    157\u001B[0m     sub_feat_names \u001B[38;5;241m=\u001B[39m feat_names[:]\n\u001B[0;32m--> 158\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_feat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_div_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43msub_feat_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_decision_tree(\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_data(train_data, best_feat, best_div_value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    163\u001B[0m         sub_feat_names, depth \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_model\n",
      "Cell \u001B[0;32mIn[10], line 158\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    156\u001B[0m     tree_model \u001B[38;5;241m=\u001B[39m {feat_name: {}}\n\u001B[1;32m    157\u001B[0m     sub_feat_names \u001B[38;5;241m=\u001B[39m feat_names[:]\n\u001B[0;32m--> 158\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_feat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_div_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43msub_feat_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_decision_tree(\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_data(train_data, best_feat, best_div_value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    163\u001B[0m         sub_feat_names, depth \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_model\n",
      "Cell \u001B[0;32mIn[10], line 158\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    156\u001B[0m     tree_model \u001B[38;5;241m=\u001B[39m {feat_name: {}}\n\u001B[1;32m    157\u001B[0m     sub_feat_names \u001B[38;5;241m=\u001B[39m feat_names[:]\n\u001B[0;32m--> 158\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_feat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_div_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mL\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43msub_feat_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_decision_tree(\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_data(train_data, best_feat, best_div_value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    163\u001B[0m         sub_feat_names, depth \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_model\n",
      "Cell \u001B[0;32mIn[10], line 161\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    157\u001B[0m     sub_feat_names \u001B[38;5;241m=\u001B[39m feat_names[:]\n\u001B[1;32m    158\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_decision_tree(\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_data(train_data, best_feat, best_div_value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    160\u001B[0m         sub_feat_names, depth \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 161\u001B[0m     tree_model[feat_name][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mN\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_feat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_div_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mR\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[43msub_feat_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_model\n",
      "Cell \u001B[0;32mIn[10], line 129\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;66;03m# 检查是否满足最小样本个数要求\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(train_data) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_samples_split:\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_major_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m# 检查是否所有实例属于同一类别\u001B[39;00m\n\u001B[1;32m    132\u001B[0m classList \u001B[38;5;241m=\u001B[39m [example[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m train_data]\n",
      "Cell \u001B[0;32mIn[10], line 115\u001B[0m, in \u001B[0;36mDecision_Tree_C45.get_major_class\u001B[0;34m(self, classList)\u001B[0m\n\u001B[1;32m    113\u001B[0m classcount \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m vote \u001B[38;5;129;01min\u001B[39;00m classList:\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m vote \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mclasscount\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey\u001B[49m():\n\u001B[1;32m    116\u001B[0m         classcount[vote] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    117\u001B[0m     classcount[vote] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'dict' object has no attribute 'key'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Add code here to load data from file and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Client_Info.csv\", encoding='gb2312')\n",
    "data = np.array(data)\n",
    "feat_names = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2024)\n",
    "#Add code here to build decision tree model using Decision_Tree_C45\n",
    "model = Decision_Tree_C45(max_depth=5, min_samples_split=10, criterion='gini')\n",
    "tree = model.create_decision_tree(train, feat_names)\n",
    "# model = Decision_Tree_C45()\n",
    "# tree = model.create_decision_tree(train, feat_names)\n",
    "pred_labels = []\n",
    "for i in range(len(test)):\n",
    "    label = model.predict(tree, feat_names, test[i])\n",
    "    pred_labels.append(label)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_C45 accuracy:%.3f\" % (acc / len(test)))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Add code here to build decision tree model using sklearn\n",
    "model_skl = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_skl.fit(train[:, 0:-1], train[:, -1])\n",
    "pred_labels_skl = model_skl.predict(test[:, 0:-1])\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_skl[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_SKL accuracy:%.3f\" % (acc / len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import necessary lib\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Decision_Tree_C45:\n",
    "    def __init__(self, max_depth=float('inf'), min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion  # \"gini\" or \"entropy\"\n",
    "\n",
    "    # define entropy calculation\n",
    "    def Entropy(self, train_data):\n",
    "        inst_num = len(train_data)  # instances number\n",
    "        label_counts = {}  # count instances of each class\n",
    "        for i in range(inst_num):\n",
    "\n",
    "            label = train_data[i][-1]  #get instance class\n",
    "            if label not in label_counts.keys():\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1  #count\n",
    "        ent = 0\n",
    "        for key in label_counts.keys():\n",
    "            #calculate each class proportion\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            ent -= prob * log(prob, 2)  # see Eq.(3.1)\n",
    "        return ent\n",
    "        #split data according to feature and feature value\n",
    "\n",
    "    def split_data(self, train_data, feature_index, feature_value, feature_type):\n",
    "        splitedData = []  # store splited data\n",
    "        if feature_type == \"D\":  # for discrete feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] == feature_value:\n",
    "                    reducedVect = []\n",
    "                    #delete used discrete feature from data\n",
    "                    for i in range(len(feat_vect)):\n",
    "                        if i < feature_index or i > feature_index:\n",
    "                            reducedVect.append(feat_vect[i])\n",
    "                    splitedData.append(reducedVect)\n",
    "        if feature_type == \"L\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] <= feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        if feature_type == \"R\":  #for continous feature\n",
    "            for feat_vect in train_data:\n",
    "                if feat_vect[feature_index] > feature_value:\n",
    "                    splitedData.append(feat_vect)\n",
    "        return splitedData\n",
    "\n",
    "    # #choose best feature to split\n",
    "    # def choose_split_feature(self, train_data, depth):\n",
    "    #     # 检查最大深度和最小样本数的条件\n",
    "    #     if depth >= self.max_depth or len(train_data) < self.min_samples_split:\n",
    "    #         return None, None\n",
    "    #\n",
    "    #     # 选择划分标准：基尼指数或熵\n",
    "    #     if self.criterion == 'gini':\n",
    "    #         base_measure = self.Gini(train_data)\n",
    "    #     else:\n",
    "    #         base_measure = self.Entropy(train_data)\n",
    "    #\n",
    "    #     feat_num = len(train_data[0]) - 1  # get available features\n",
    "    #     base_ent = self.Entropy(train_data)\n",
    "    #     bestInforGain = 0.0\n",
    "    #     best_feat_index = -1\n",
    "    #     best_feat_value = 0\n",
    "    #     for i in range(feat_num):\n",
    "    #         if isinstance(train_data[0][i], str):  #for discrete feature\n",
    "    #             feat_list = [example[i] for example in train_data]\n",
    "    #             unique_values = set(feat_list)\n",
    "    #             newEnt = 0\n",
    "    #             for value in unique_values:\n",
    "    #                 sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "    #                 prop = float(len(sub_data)) / len(train_data)\n",
    "    #                 newEnt += prop * self.Entropy(sub_data)  #see Eq.(3.2)\n",
    "    #                 inforgain = base_ent - newEnt\n",
    "    #                 if inforgain > bestInforGain:\n",
    "    #                     best_feat_index = i\n",
    "    #                     bestInforGain = inforgain\n",
    "    #             else:  #for continous feature\n",
    "    #                 feat_list = [example[i] for example in train_data]\n",
    "    #                 unique_values = set(feat_list)\n",
    "    #                 sort_unique_values = sorted(unique_values)\n",
    "    #                 minEnt = np.inf\n",
    "    #                 for j in range(len(sort_unique_values) - 1):\n",
    "    #                     div_value = (sort_unique_values[j] + sort_unique_values[j + 1]) / 2\n",
    "    #                     sub_data_left = self.split_data(train_data, i, div_value, \"L\")\n",
    "    #                     sub_data_right = self.split_data(train_data, i, div_value, \"R\")\n",
    "    #                     prop_left = float(len(sub_data_left)) / len(train_data)\n",
    "    #                     prop_right = float(len(sub_data_right)) / len(train_data)\n",
    "    #                     ent = prop_left * self.Entropy(sub_data_left) + \\\n",
    "    #                           prop_right * self.Entropy(sub_data_right)  #see Eq.(3.6)\n",
    "    #                     if ent < minEnt:\n",
    "    #                         minEnt = ent\n",
    "    #                         best_feat_value = div_value\n",
    "    #                 inforgain = base_ent - minEnt\n",
    "    #                 if inforgain > bestInforGain:\n",
    "    #                     bestInforGain = inforgain\n",
    "    #                     best_feat_index = i\n",
    "    #         return best_feat_index, best_feat_value\n",
    "\n",
    "\n",
    "    def choose_split_feature(self, train_data, depth):\n",
    "        # 检查最大深度和最小样本数的条件\n",
    "        if depth >= self.max_depth or len(train_data) < self.min_samples_split:\n",
    "            return None, None\n",
    "\n",
    "        feat_num = len(train_data[0]) - 1  # 获取可用特征的数量\n",
    "        bestInforGain = 0.0\n",
    "        best_feat_index = -1\n",
    "        best_feat_value = None\n",
    "\n",
    "        # 根据选择的标准计算基础度量（基尼指数或熵）\n",
    "        if self.criterion == 'gini':\n",
    "            base_measure = self.Gini(train_data)\n",
    "        else:\n",
    "            base_measure = self.Entropy(train_data)\n",
    "\n",
    "        for i in range(feat_num):\n",
    "            feat_list = [example[i] for example in train_data]\n",
    "            unique_values = set(feat_list)\n",
    "\n",
    "            for value in unique_values:\n",
    "                sub_data = self.split_data(train_data, i, value, \"D\")\n",
    "                prop = float(len(sub_data)) / len(train_data)\n",
    "                if self.criterion == 'gini':\n",
    "                    new_measure = prop * self.Gini(sub_data)\n",
    "                else:\n",
    "                    new_measure = prop * self.Entropy(sub_data)\n",
    "\n",
    "                inforgain = base_measure - new_measure\n",
    "                if inforgain > bestInforGain:\n",
    "                    bestInforGain = inforgain\n",
    "                    best_feat_index = i\n",
    "                    best_feat_value = value\n",
    "\n",
    "        return best_feat_index, best_feat_value\n",
    "\n",
    "    # get major class\n",
    "    def get_major_class(self, classList):\n",
    "        classcount = {}\n",
    "        for vote in classList:\n",
    "            if vote not in classcount:\n",
    "                classcount[vote] = 0\n",
    "            classcount[vote] += 1\n",
    "\n",
    "        sortedclasscount = sorted(classcount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        major = sortedclasscount[0][0]\n",
    "        return major\n",
    "\n",
    "    # # create decision tree\n",
    "    # def create_decision_tree(self, train_data, feat_names):\n",
    "    #     classList = [example[-1] for example in train_data]\n",
    "    #     if classList.count(classList[0]) == len(classList):  #see condition A\n",
    "    #         return classList[0]\n",
    "    #     if len(train_data[0]) == 1:  #see condition B\n",
    "    #         return self.get_major_class(classList)\n",
    "    #     if len(train_data) == 0:  #see condition C\n",
    "    #         return\n",
    "    #     # choose best division feature\n",
    "    #     best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "    #     if isinstance(train_data[0][best_feat], str):  # for discrete feature\n",
    "    #         feat_name = feat_names[best_feat]\n",
    "    #         tree_model = {feat_name: {}}  # generate a root node\n",
    "    #         del (feat_names[best_feat])  # del feature used\n",
    "    #         feat_values = [example[best_feat] for example in train_data]\n",
    "    #         unique_feat_values = set(feat_values)\n",
    "    #         #create a node for each value of the best feature\n",
    "    #         for value in unique_feat_values:\n",
    "    #             sub_feat_names = feat_names[:]\n",
    "    #             tree_model[feat_name][value] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                      best_feat, value, \"D\"),\n",
    "    #                                                                      sub_feat_names)\n",
    "    #     else:  #for contiunous feature\n",
    "    #         best_feat_name = feat_names[best_feat] + \"<\" + str(best_div_value)\n",
    "    #         tree_model = {best_feat_name: {}}  # generate a root node\n",
    "    #         sub_feat_names = feat_names\n",
    "    #         # generate left node\n",
    "    #         tree_model[best_feat_name][\"Y\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                     best_feat,\n",
    "    #                                                                                     best_div_value, \"L\"),\n",
    "    #                                                                     sub_feat_names)\n",
    "    #         #generate right node\n",
    "    #         tree_model[best_feat_name][\"N\"] = self.create_decision_tree(self.split_data(train_data,\n",
    "    #                                                                                     best_feat,\n",
    "    #                                                                                     best_div_value, \"R\"),\n",
    "    #                                                                     sub_feat_names)\n",
    "    #     return tree_model\n",
    "    def create_decision_tree(self, train_data, feat_names, depth=0):\n",
    "        # 检查是否所有的类标签都相同\n",
    "        classList = [example[-1] for example in train_data]\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "\n",
    "        # 检查是否还有可用的特征或者是否达到最大深度\n",
    "        if len(train_data[0]) == 1 or depth >= self.max_depth:\n",
    "            return self.get_major_class(classList)\n",
    "\n",
    "        # 选择最佳分割特征\n",
    "        best_feat, best_div_value = self.choose_split_feature(train_data)\n",
    "        if best_feat is None:  # 如果没有更多特征可用于进一步分割\n",
    "            return self.get_major_class(classList)\n",
    "\n",
    "        # 根据最佳特征的类型进行分割\n",
    "        if isinstance(train_data[0][best_feat], str):  # 离散特征\n",
    "            feat_name = feat_names[best_feat]\n",
    "            tree_model = {feat_name: {}}\n",
    "            del feat_names[best_feat]  # 删除已使用的特征\n",
    "            unique_feat_values = set([example[best_feat] for example in train_data])\n",
    "            for value in unique_feat_values:\n",
    "                sub_feat_names = feat_names[:]  # 复制特征列表\n",
    "                sub_data = self.split_data(train_data, best_feat, value, \"D\")\n",
    "                tree_model[feat_name][value] = self.create_decision_tree(sub_data, sub_feat_names, depth + 1)\n",
    "        else:  # 连续特征\n",
    "            feat_name = feat_names[best_feat] + \"<=\" + str(best_div_value)\n",
    "            tree_model = {feat_name: {}}\n",
    "            sub_feat_names = feat_names[:]\n",
    "            # 分割为左右子树\n",
    "            left_subtree = self.split_data(train_data, best_feat, best_div_value, \"L\")\n",
    "            right_subtree = self.split_data(train_data, best_feat, best_div_value, \"R\")\n",
    "            tree_model[feat_name][\"yes\"] = self.create_decision_tree(left_subtree, sub_feat_names, depth + 1)\n",
    "            tree_model[feat_name][\"no\"] = self.create_decision_tree(right_subtree, sub_feat_names, depth + 1)\n",
    "\n",
    "        return tree_model\n",
    "\n",
    "    #define predict function\n",
    "    def predict(self, tree_model, feat_names, feat_vect):\n",
    "        firstStr = list(tree_model.keys())[0]  # get tree root\n",
    "        lessIndex = str(firstStr).find('<')\n",
    "        if lessIndex > -1:  # if root is a continous feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            feat_name = str(firstStr)[:lessIndex]\n",
    "            featIndex = feat_names.index(feat_name)\n",
    "            div_value = float(str(firstStr)[lessIndex + 1:])\n",
    "            if feat_vect[featIndex] <= div_value:\n",
    "                if isinstance(secondDict[\"Y\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"Y\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"Y\"]\n",
    "            else:\n",
    "                if isinstance(secondDict[\"N\"], dict):\n",
    "                    classLabel = self.predict(secondDict[\"N\"],\n",
    "                                              feat_names, feat_vect)\n",
    "                else:\n",
    "                    classLabel = secondDict[\"N\"]\n",
    "            return classLabel\n",
    "        else:  #if root is a discrete feature\n",
    "            # recursively search untill leaft node\n",
    "            secondDict = tree_model[firstStr]\n",
    "            featIndex = feat_names.index(firstStr)\n",
    "            key = feat_vect[featIndex]\n",
    "            valueOfFeat = secondDict[key]\n",
    "            if isinstance(valueOfFeat, dict):\n",
    "                classLabel = self.predict(valueOfFeat, feat_names, feat_vect)\n",
    "            else:\n",
    "                classLabel = valueOfFeat\n",
    "            return classLabel\n",
    "\n",
    "    def Gini(self, train_data):\n",
    "        inst_num = len(train_data)\n",
    "        label_counts = {}\n",
    "        for feat_vect in train_data:\n",
    "            label = feat_vect[-1]\n",
    "            if label not in label_counts:\n",
    "                label_counts[label] = 0\n",
    "            label_counts[label] += 1\n",
    "        gini = 1.0\n",
    "        for key in label_counts:\n",
    "            prob = float(label_counts[key]) / inst_num\n",
    "            gini -= prob ** 2\n",
    "        return gini\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Decision_Tree_C45' object has no attribute 'split_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#Add code here to build decision tree model using Decision_Tree_C45\u001B[39;00m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m Decision_Tree_C45(max_depth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, min_samples_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, criterion\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgini\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m tree \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_decision_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# model = Decision_Tree_C45()\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# tree = model.create_decision_tree(train, feat_names)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m pred_labels \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[0;32mIn[2], line 140\u001B[0m, in \u001B[0;36mDecision_Tree_C45.create_decision_tree\u001B[0;34m(self, train_data, feat_names, depth)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_major_class(classList)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;66;03m# 选择最佳分裂属性\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m best_feat, best_div_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_split_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m# 处理离散特征和连续特征的情况\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(train_data[\u001B[38;5;241m0\u001B[39m][best_feat], \u001B[38;5;28mstr\u001B[39m):\n",
      "Cell \u001B[0;32mIn[2], line 74\u001B[0m, in \u001B[0;36mDecision_Tree_C45.choose_split_feature\u001B[0;34m(self, train_data)\u001B[0m\n\u001B[1;32m     72\u001B[0m feature_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m([example[i] \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m train_data])\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split_val \u001B[38;5;129;01min\u001B[39;00m feature_values:\n\u001B[0;32m---> 74\u001B[0m     left_subset, right_subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit_dataset\u001B[49m(train_data, i, split_val)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(left_subset) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(right_subset) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Decision_Tree_C45' object has no attribute 'split_dataset'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Add code here to load data from file and preprocess data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Client_Info.csv\", encoding='gb2312')\n",
    "data = np.array(data)\n",
    "feat_names = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=2021)\n",
    "#Add code here to build decision tree model using Decision_Tree_C45\n",
    "model = Decision_Tree_C45(max_depth=5, min_samples_split=10, criterion='gini')\n",
    "tree = model.create_decision_tree(train, feat_names)\n",
    "# model = Decision_Tree_C45()\n",
    "# tree = model.create_decision_tree(train, feat_names)\n",
    "pred_labels = []\n",
    "for i in range(len(test)):\n",
    "    label = model.predict(tree, feat_names, test[i])\n",
    "    pred_labels.append(label)\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_C45 accuracy:%.3f\" % (acc / len(test)))\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Add code here to build decision tree model using sklearn\n",
    "model_skl = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_skl.fit(train[:, 0:-1], train[:, -1])\n",
    "pred_labels_skl = model_skl.predict(test[:, 0:-1])\n",
    "acc = 0\n",
    "for i in range(len(test)):\n",
    "    if pred_labels_skl[i] == test[i, -1]:\n",
    "        acc += 1.0\n",
    "print(\"Decision_Tree_SKL accuracy:%.3f\" % (acc / len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
