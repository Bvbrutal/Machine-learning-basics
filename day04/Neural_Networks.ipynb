{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 神经网络模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,layer_sizes):\n",
    "        self.num_layers = len(layer_sizes) # layer number of NN\n",
    "        self.layers = layer_sizes # node numbers of each layer\n",
    "        #initialize connenct weights of layers\n",
    "        self.weights = [np.random.randn(y,x) for x, y in zip(layer_sizes[:-1],layer_sizes[1:])]\n",
    "        #initialize biases of each layer(input layer has no bias)\n",
    "        self.biases = [np.random.randn(y,1) for y in layer_sizes[1:]]\n",
    "    #sigmoid activation function\n",
    "    def sigmoid(self,z):\n",
    "        act = 1.0/(1.0 + np.exp(-z))\n",
    "        #act=np.exp(z)/(1+np.exp(z))\n",
    "        return act\n",
    "    # derivative function of sigmoid activation function\n",
    "    def sigmoid_prime(self,z):\n",
    "        act = self.sigmoid(z)*(1.0-self.sigmoid(z))\n",
    "        return act\n",
    "    # feed forward to get prediction\n",
    "    def feed_forward(self,x):\n",
    "        output = x.copy()\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            output = self.sigmoid(np.dot(w,output)+b)\n",
    "        return output\n",
    "    # feed backward to update NN paremeters\n",
    "    def feed_backward(self,x,y):\n",
    "        delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #activations of input layer\n",
    "        activation = np.transpose(x)\n",
    "        activations = [activation]\n",
    "        # input after input layer\n",
    "        layer_input = []\n",
    "        #forward to get each layer’s input and output\n",
    "        for b, w in zip(self.biases,self.weights):\n",
    "            z = np.dot(w,activation) + b\n",
    "            layer_input.append(z) #input of each layer\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)#output of each layer\n",
    "        #loss funtion\n",
    "        ground_truth = np.transpose(y)\n",
    "        diff = activations[-1] - ground_truth\n",
    "        #get input of last layer\n",
    "        last_layer_input = layer_input[-1]\n",
    "        delta = np.multiply(diff,self.sigmoid_prime(last_layer_input))\n",
    "        #bias update of last layer\n",
    "        delta_b[-1] = np.sum(delta,axis=1,keepdims=True)\n",
    "        #weight update of last layer\n",
    "        delta_w[-1] = np.dot(delta, np.transpose(activations[-2]))\n",
    "        #update weights and bias from 2nd layer to last layer\n",
    "        for i in range(2,self.num_layers):\n",
    "            input_values = layer_input[-i]\n",
    "            delta = np.multiply(np.dot(np.transpose(self.weights[-i+1]),delta),self.sigmoid_prime(input_values))\n",
    "            delta_b[-i] = np.sum(delta,axis=1,keepdims=True)\n",
    "            delta_w[-i] = np.dot(delta,np.transpose(activations[-i-1]))\n",
    "        return delta_b,delta_w\n",
    "    #training using BP\n",
    "    def fit(self, x,y,learnrate,mini_batch_size, epochs=1000):\n",
    "        n = len(x)#training size\n",
    "        for i in range(epochs):\n",
    "            randomlist = np.random.randint(0,n-mini_batch_size,int(n/mini_batch_size))\n",
    "            batch_x = [x[k:k+mini_batch_size] for k in randomlist]\n",
    "            batch_y = [y[k:k+mini_batch_size] for k in randomlist]\n",
    "            for j in range(len(batch_x)):\n",
    "                delta_b,delta_w = self.feed_backward(batch_x[j], batch_y[j])\n",
    "                self.weights = [w - (learnrate/mini_batch_size)*dw for w, dw in\n",
    "                                zip(self.weights,delta_w)]\n",
    "                self.biases = [b - (learnrate/mini_batch_size)*db for b, db in\n",
    "                               zip(self.biases,delta_b)]\n",
    "            if (i+1)%100 == 0:\n",
    "                labels = self.predict(x)\n",
    "                acc = 0.0\n",
    "                for k in range(len(labels)):\n",
    "                    if y[k,labels[k]]==1.0:\n",
    "                        acc += 1.0\n",
    "                acc=acc/len(labels)\n",
    "                print(\"iterations %d accuracy %.3f\"%(i+1,acc))\n",
    "    #predict function\n",
    "    def predict(self, x):\n",
    "        results = self.feed_forward(x.T)\n",
    "        labels = [np.argmax(results[:,y]) for y in range(results.shape[1])]\n",
    "        return labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#实践部分\n",
    "import struct\n",
    "import os\n",
    "#fucntion to load MNIST data\n",
    "def load_mnist_data(path,kind='train'):\n",
    "    label_path = os.path.join(path, '%s-labels.idx1-ubyte'%kind)\n",
    "    image_path = os.path.join(path, '%s-images.idx3-ubyte'%kind)\n",
    "    with open(label_path,'rb') as lbpath: # open label file\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,dtype=np.uint8)\n",
    "    with open(image_path,'rb') as imgpath:# open image file\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        #transform image into 784-dimensional feature vector\n",
    "        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels),784)\n",
    "    return images,labels\n",
    "\n",
    "#定义图片显示函数\n",
    "import matplotlib.pyplot as plt\n",
    "def show_image(image):\n",
    "    plt.figure()\n",
    "    img = image.reshape(28,28)\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()\n",
    "#数据读取和预处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "path = 'MNIST'\n",
    "train_images, train_labels = load_mnist_data(path,kind='train')\n",
    "# show_image(train_images)\n",
    "train_y = np.zeros((len(train_labels),10))\n",
    "for i in range(len(train_labels)):\n",
    "    train_y[i,train_labels[i]]=1\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_images)\n",
    "test_images, test_labels = load_mnist_data(path,kind='t10k')\n",
    "# show_image(test_images)\n",
    "test_y = np.zeros((len(test_labels),10))\n",
    "for i in range(len(test_labels)):\n",
    "    test_y[i,test_labels[i]]=1\n",
    "test_x = scaler.fit_transform(test_images)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/vg1kz23x7hj4xwwxmsrl6w7c0000gq/T/ipykernel_1355/1931061067.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  act = 1.0/(1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 100 accuracy 0.331\n",
      "iterations 200 accuracy 0.501\n",
      "iterations 300 accuracy 0.542\n",
      "iterations 400 accuracy 0.556\n",
      "iterations 500 accuracy 0.563\n",
      "iterations 600 accuracy 0.567\n",
      "iterations 700 accuracy 0.570\n",
      "iterations 800 accuracy 0.572\n",
      "iterations 900 accuracy 0.574\n",
      "iterations 1000 accuracy 0.576\n",
      "test accuracy:0.572\n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "layer_sizes = [784,100,10]\n",
    "NN = NeuralNetwork(layer_sizes)\n",
    "NN.fit(train_x, train_y, learnrate=0.01, mini_batch_size=100,epochs=1000)\n",
    "#测试\n",
    "test_pred_labels = NN.predict(test_x)\n",
    "acc = 0.0\n",
    "for k in range(len(test_pred_labels)):\n",
    "    if test_y[k,test_pred_labels[k]]==1.0:\n",
    "        acc += 1.0\n",
    "acc=acc/len(test_pred_labels)\n",
    "print(\"test accuracy:%.3f\"%(acc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sklearn实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:0.961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100),\n",
    "                      solver='sgd',batch_size=100,learning_rate='constant',\n",
    "                      learning_rate_init=0.01,max_iter=3000)\n",
    "model.fit(train_x, train_y)\n",
    "labels = model.predict(test_x)\n",
    "acc = 0.0\n",
    "for k in range(len(labels)):\n",
    "    index = 0\n",
    "    for j in range(10):\n",
    "        if labels[k,j]==1:\n",
    "            index = j\n",
    "            break\n",
    "    if test_y[k,index]==1.0:\n",
    "        acc += 1.0\n",
    "acc=acc/len(labels)\n",
    "print(\"test accuracy:%.3f\"%(acc))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
